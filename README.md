# EmotionGestures
### Code Pytorch Implementation

[EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture Generation](https://arxiv.org/abs/2305.18891)

[Xingqun Qi](https://scholar.google.com.hk/citations?hl=zh-CN&user=3tO41a8AAAAJ&view_op=list_works&sortby=pubdate), [Chen Liu](https://scholar.google.com/citations?hl=zh-CN&user=HmvE2WsAAAAJ&view_op=list_works&sortby=pubdate), [Lincheng Li](https://scholar.google.com/citations?user=NYLsVscAAAAJ&hl=en), Jie Hou, Haoran Xin, [Xin Yu](https://scholar.google.com/citations?user=oxdtuSEAAAAJ&hl=en)

## Dataset
[Our newly collected TED Emotion Dataset is available now.](https://drive.google.com/file/d/1Yzks2FjbIg0n94y2rVw3plI0sq7Wpy8L/view?usp=sharing)
## Code
We are organizing our code and will release the code soon. Sorry for the delay.
## Acknowledgement
Thanks for the pioneering works [Trimodal](https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context) and [Frankmocap](https://github.com/facebookresearch/frankmocap).

